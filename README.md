# IA PBPM API Gateway

Um Gateway de API inteligente e seguro para LLMs locais rodando via **Ollama**. Este projeto atua como um "Guard" que gerencia autentica√ß√£o, contagem de tokens, logs de auditoria e, crucialmente, o **gerenciamento de mem√≥ria VRAM** para alternar entre modelos leves e pesados automaticamente.

## üöÄ Funcionalidades

- **Autentica√ß√£o Segura:** Sistema de chave Mestra (Admin) e chaves de Clientes (Usu√°rios).
- **Gest√£o Inteligente de Mem√≥ria:**
  - Mant√©m modelos leves (ex: `llama3.2:3b`) sempre carregados na mem√≥ria.
  - Alterna automaticamente modelos pesados (ex: `qwen2.5:32b`, `llama3:70b`) para evitar estouro de VRAM (OOM), descarregando o anterior antes de carregar o novo.
- **Auditoria de Tokens:** Registra o consumo (prompt e resposta) e timestamp de cada requisi√ß√£o em banco de dados SQLite.
- **Auto-Discovery:** Detecta automaticamente quais modelos o Ollama possui instalados.
- **Endpoint de Preload:** Permite "aquecer" um modelo pesado antes da infer√™ncia real.

## üõ†Ô∏è Instala√ß√£o e Configura√ß√£o

### Pr√©-requisitos
- Docker e Docker Compose instalados.

### 1. Configura√ß√£o do Ambiente
Crie um arquivo `.env` na raiz do projeto (baseado nas configura√ß√µes do `docker-compose.yml` e `main.py`):

```bash

# URL interna do Ollama (padr√£o docker)
OLLAMA_URL=http://ollama:11434

# Modelos que devem ficar SEMPRE na mem√≥ria (separados por v√≠rgula)
ALWAYS_ON_MODELS=llama3.2:3b,llama3:8b
# IA PBPM API Gateway

Um Gateway de API inteligente e seguro para LLMs locais rodando via **Ollama**. Este projeto atua como um "Guard" que gerencia autentica√ß√£o, contagem de tokens, logs de auditoria e, crucialmente, o **gerenciamento de mem√≥ria VRAM** para alternar entre modelos leves e pesados automaticamente.

## üöÄ Funcionalidades

- **Autentica√ß√£o Segura:** Sistema de chave Mestra (Admin) e chaves de Clientes (Usu√°rios).
- **Gest√£o Inteligente de Mem√≥ria:**
  - Mant√©m modelos leves (ex: `llama3.2:3b`) sempre carregados na mem√≥ria.
  - Alterna automaticamente modelos pesados (ex: `qwen2.5:32b`, `llama3:70b`) para evitar estouro de VRAM (OOM), descarregando o anterior antes de carregar o novo.
- **Auditoria de Tokens:** Registra o consumo (prompt e resposta) e timestamp de cada requisi√ß√£o em banco de dados SQLite.
- **Auto-Discovery:** Detecta automaticamente quais modelos o Ollama possui instalados.
- **Endpoint de Preload:** Permite "aquecer" um modelo pesado antes da infer√™ncia real.

## üõ†Ô∏è Instala√ß√£o e Configura√ß√£o

### Pr√©-requisitos
- Docker e Docker Compose instalados.

### 1. Configura√ß√£o do Ambiente
Crie um arquivo `.env` na raiz do projeto (baseado nas configura√ß√µes do `docker-compose.yml` e `main.py`):

```bash

# URL interna do Ollama (padr√£o docker)
OLLAMA_URL=http://ollama:11434

# Modelos que devem ficar SEMPRE na mem√≥ria (separados por v√≠rgula)
ALWAYS_ON_MODELS=llama3.2:3b,llama3:8b

üìö Documenta√ß√£o da API
üîê 1. Administrativo (Requer MASTER_API_KEY)
Criar Nova Chave de Cliente
Gera uma chave de acesso para uso em infer√™ncias.

POST /admin/create_key

Headers: Authorization: Bearer SUA_MASTER_KEY

Query Params:

name: Nome do cliente

email: Email do cliente

Exemplo de Resposta:

JSON

{
  "message": "Chave criada com sucesso",
  "api_key": "pbpm-a1b2c3d4...", 
  "owner": "cliente@email.com"
}
üß† 2. Gest√£o de Modelos (Requer Chave de Cliente)
Listar Modelos Dispon√≠veis
Mostra quais modelos est√£o instalados e sua categoria (always_on ou on_demand).

GET /api/available_models

Headers: Authorization: Bearer CHAVE_DO_CLIENTE

Preload (Aquecimento)
Avisa a API para carregar um modelo pesado na mem√≥ria, descarregando outros se necess√°rio.

POST /preload

Headers: Authorization: Bearer CHAVE_DO_CLIENTE

Body:

JSON

{
  "model": "qwen2.5-coder:32b"
}
üí¨ 3. Infer√™ncia (Chat)
Compat√≠vel com a API padr√£o do Ollama. O sistema intercepta, autentica, loga os tokens e gerencia a mem√≥ria antes de repassar ao Ollama.

POST /api/chat (ou /api/generate)

Headers: Authorization: Bearer CHAVE_DO_CLIENTE

Body:

JSON

{
  "model": "llama3.2:3b",
  "messages": [
    { "role": "user", "content": "Ol√°, como voc√™ est√°?" }
  ],
  "stream": true
}
üìÇ Estrutura de Arquivos
app/main.py: C√≥digo principal da API (FastAPI).

app/Dockerfile: Defini√ß√£o da imagem Docker.

docker-compose.yml: Orquestra√ß√£o dos servi√ßos.

data/guard.db: Banco de dados SQLite (criado automaticamente, persistido via volume).

‚ö†Ô∏è Notas Importantes
A Chave Mestra definida no .env serve apenas para criar novas chaves. Ela n√£o funciona para endpoints de chat.

O sistema suporta um limite de mem√≥ria configurado no docker-compose.yml (padr√£o 26GB). A l√≥gica de "Heavy Swap" garante que dois modelos pesados n√£o concorram por esse espa√ßo.